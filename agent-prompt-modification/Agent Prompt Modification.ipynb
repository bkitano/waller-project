{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents Prompt Modification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate an agent that changes the prompt of another agent. This would constitute a kind of quality intelligence improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an evaluator agent evaluates the performance of a task agent\n",
    "# the evaluator agent stores a prompt for a task agent in a file\n",
    "# the task agent performs a task and stores a transcript of its actions to a file\n",
    "# the evaluator reads the transcript and evaluates the task agent's performance\n",
    "# the evaluator decides how to change the task agent's prompt based on the performance\n",
    "# the task agent reads the new prompt and performs the task again"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to work on the 20_newsgroups dataset for classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SerpAPIWrapper\n",
    "from langchain.agents import AgentExecutor, OpenAIFunctionsAgent\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Type\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "search = SerpAPIWrapper(serpapi_api_key=os.environ['SERPAPI_API_KEY'])\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo-0613', openai_api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "class EvaluateSingleTranscriptToolInput(BaseModel):\n",
    "    task_agent_prompt: str = Field(description=\"The prompt used by the task agent to perform the task.\")\n",
    "    transcript: str = Field(description=\"The transcript of a task agent's actions.\")\n",
    "\n",
    "class EvaluateSingleTranscriptTool(BaseTool):\n",
    "    \"\"\"\n",
    "    This evaluates a single transcript of a task agent's actions. It tries to assess whether\n",
    "    the task agent's actions were successful or not. It could be a simple loss function, or\n",
    "    it could be a more complex function that tries to determine the cause of the task agent's\n",
    "    failure.\n",
    "    \"\"\"\n",
    "    name = \"transcript-evaluator\"\n",
    "    description = \"Evaluate the transcript of a task agent's actions.\"\n",
    "    args_schema: Type[BaseTool] = EvaluateSingleTranscriptToolInput\n",
    "\n",
    "    def _run(self, transcript: str, prmopt: str):\n",
    "        pass # could be some sort of loss function?\n",
    "    def _arun(self, transcript: str, prmopt: str):\n",
    "        pass\n",
    "\n",
    "class ModifyPromptToolInput(BaseModel):\n",
    "\n",
    "    task_agent_prompt: str = Field(description=\"The prompt used by the task agent to perform the task.\")\n",
    "    evaluation: str = Field(description=\"The evaluation of the task agent's performance.\")\n",
    "\n",
    "class ModifyPromptTool(BaseTool):\n",
    "    \"\"\"\n",
    "    Seems like it's going to be limited if it can only modify the prompt based on a single transcript.\n",
    "    Probably the EvaluateTranscriptsTool needs to be able to evaluate multiple transcripts, then\n",
    "    come up with an abstraction for a common cause as to why the task agent failed, then modify the prompt\n",
    "    based on that abstraction.\n",
    "    \"\"\"\n",
    "    name = \"prompt-modifier\"\n",
    "    description = \"Modify the prompt of a task agent based on its performance.\"\n",
    "    args_schema: Type[BaseModel] = ModifyPromptToolInput\n",
    "    def _run(self, evaluation: str, task_agent_prompt: str):\n",
    "        prompt = PromptTemplate(input_variables=[\n",
    "            'task_agent_prompt',\n",
    "            'transcript',\n",
    "            'evaluation'\n",
    "        ], template=\"\"\"\n",
    "        You are an evaluator of AI agents. You review the diagnosed problem of an agent and decide how to change their prompts.\n",
    "        The task agent's original prompt was: {task_agent_prompt}\n",
    "        The task agent's transcript is: {transcript}\n",
    "        The task agent's evaluation is: {evaluation}\n",
    "\n",
    "        You decide to change the task agent's prompt to:\n",
    "        \"\"\")\n",
    "        chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        return chain.run({'task_agent_prompt': task_agent_prompt, 'evaluation': evaluation})\n",
    "\n",
    "    def _arun(self, evaluation: str, transcript: str, prmopt: str):\n",
    "        pass\n",
    "\n",
    "tools = [\n",
    "    EvaluateSingleTranscriptTool(),\n",
    "    ModifyPromptTool()\n",
    "]\n",
    "\n",
    "evaluator_agent = OpenAIFunctionsAgent.from_llm_and_tools(\n",
    "    llm=llm, \n",
    "    tools=tools,\n",
    "    prefix=\"You are an evaluator of AI agents. You review task agents' transcripts and decide how to change their prompts.\"\n",
    "    )\n",
    "\n",
    "evaluator_agent_executor = AgentExecutor.from_agent_and_tools(evaluator_agent, tools)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_agent_prefix = open('./task_agent_prefix.txt', 'r').read()\n",
    "\n",
    "class ClassifyArticleToolInput(BaseModel):\n",
    "    article: str = Field(description=\"The article to classify.\")\n",
    "\n",
    "class ClassifyArticleTool(BaseTool):\n",
    "    \"\"\"\n",
    "    This tool classifies an article based on its content.\n",
    "    \"\"\"\n",
    "    name = \"article-classifier\"\n",
    "    description = \"Classify an article based on its content.\"\n",
    "    args_schema: Type[BaseTool] = ClassifyArticleToolInput\n",
    "\n",
    "    def _run(self, article: str):\n",
    "        prompt = PromptTemplate(input_variables=['article'], template=\"\"\"\n",
    "                                You are a classifier of articles. You classify articles based on their content.\n",
    "                                The article you are classifying is: {article}\n",
    "                                You classify the article as:\n",
    "                                \"\"\")\n",
    "        chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        return chain.run({'article': article})\n",
    "\n",
    "    def _arun(self, article: str):\n",
    "        pass\n",
    "\n",
    "task_agent_tools = [ClassifyArticleTool()]\n",
    "task_agent = OpenAIFunctionsAgent.from_llm_and_tools(\n",
    "    llm=llm,\n",
    "    tools=task_agent_tools,\n",
    "    prefix=task_agent_prefix,\n",
    "    verbose=True\n",
    "    )\n",
    "\n",
    "task_agent_executor = AgentExecutor.from_agent_and_tools(task_agent, task_agent_tools, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep 20_newsgroups dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep the 20_newsgroups dataset\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def get_data():\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "    newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "    X_train = pd.DataFrame(newsgroups_train.data)\n",
    "    X_train.columns = ['text']\n",
    "    X_test = pd.DataFrame(newsgroups_test.data)\n",
    "    X_test.columns = ['text']\n",
    "    X_train['text'] = X_train['text'].apply(clean_text)\n",
    "    X_train['target'] = newsgroups_train.target\n",
    "    X_train['label'] = X_train['target'].apply(lambda x: newsgroups_train.target_names[x])\n",
    "    \n",
    "    X_test['text'] = X_test['text'].apply(clean_text)\n",
    "    X_test['target'] = newsgroups_test.target\n",
    "    X_test['label'] = X_test['target'].apply(lambda x: newsgroups_test.target_names[x])\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "X_train, X_test = get_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the task agent on an example article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the article, it seems that you are looking for information about a car called the Bricklin. The car is described as a 2-door sports car from the late 60s or early 70s. It is known for having small doors and a separate front bumper from the rest of the body. \n",
      "\n",
      "Unfortunately, the article does not provide any additional information about the model name, engine specs, years of production, where the car is made, or its history. If you have any further information or if anyone can provide more details about the Bricklin, please email the requester.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    sample = dict(X_train.iloc[i])\n",
    "    text = sample['text']\n",
    "    label = sample['label']\n",
    "\n",
    "    print(task_agent_executor.run(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waller-project-_ugCvovn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
