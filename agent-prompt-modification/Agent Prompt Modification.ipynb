{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents Prompt Modification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate an agent that changes the prompt of another agent. This would constitute a kind of quality intelligence improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an evaluator agent evaluates the performance of a task agent\n",
    "# the evaluator agent stores a prompt for a task agent in a file\n",
    "# the task agent performs a task and stores a transcript of its actions to a file\n",
    "# the evaluator reads the transcript and evaluates the task agent's performance\n",
    "# the evaluator decides how to change the task agent's prompt based on the performance\n",
    "# the task agent reads the new prompt and performs the task again"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to work on the 20_newsgroups dataset for classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 42) (1901114429.py, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 42\u001b[0;36m\u001b[0m\n\u001b[0;31m    Seems like it's going to be limited if it can only modify the prompt based on a single transcript.\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 42)\n"
     ]
    }
   ],
   "source": [
    "from langchain import SerpAPIWrapper\n",
    "from langchain.agents import AgentExecutor, initialize_agent, AgentType, OpenAIFunctionsAgent\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Type\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "search = SerpAPIWrapper(serpapi_api_key=os.environ['SERPAPI_API_KEY'])\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo-0613', openai_api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "def evaluate():\n",
    "    pass\n",
    "\n",
    "class EvaluateSingleTranscriptToolInput(BaseModel):\n",
    "    task_agent_prompt: str = Field(description=\"The prompt used by the task agent to perform the task.\")\n",
    "    transcript: str = Field(description=\"The transcript of a task agent's actions.\")\n",
    "\n",
    "class EvaluateSingleTranscriptTool(BaseTool):\n",
    "    \"\"\"\n",
    "    This evaluates a single transcript of a task agent's actions. It tries to assess whether\n",
    "    the task agent's actions were successful or not. It could be a simple loss function, or\n",
    "    it could be a more complex function that tries to determine the cause of the task agent's\n",
    "    failure.\n",
    "    \"\"\"\n",
    "    name = \"Evaluate Transcript\"\n",
    "    description = \"Evaluate the transcript of a task agent's actions.\"\n",
    "    args_schema: Type[BaseTool] = EvaluateSingleTranscriptToolInput\n",
    "\n",
    "    def _run(self, transcript: str, prmopt: str):\n",
    "        pass # could be some sort of loss function?\n",
    "    def _arun(self, transcript: str, prmopt: str):\n",
    "        pass\n",
    "\n",
    "class ModifyPromptToolInput(BaseModel):\n",
    "\n",
    "    task_agent_prompt: str = Field(description=\"The prompt used by the task agent to perform the task.\")\n",
    "    evaluation: str = Field(description=\"The evaluation of the task agent's performance.\")\n",
    "\n",
    "class ModifyPromptTool(BaseTool):\n",
    "    \"\"\"\n",
    "    Seems like it's going to be limited if it can only modify the prompt based on a single transcript.\n",
    "    Probably the EvaluateTranscriptsTool needs to be able to evaluate multiple transcripts, then\n",
    "    come up with an abstraction for a common cause as to why the task agent failed, then modify the prompt\n",
    "    based on that abstraction.\n",
    "    \"\"\"\n",
    "    name = \"Modify Prompt\"\n",
    "    description = \"Modify the prompt of a task agent based on its performance.\"\n",
    "    args_schema: Type[BaseModel] = ModifyPromptToolInput\n",
    "    def _run(self, evaluation: str, task_agent_prompt: str):\n",
    "        prompt = PromptTemplate(input_variables=[\n",
    "            'task_agent_prompt',\n",
    "            'transcript',\n",
    "            'evaluation'\n",
    "        ], template=\"\"\"\n",
    "        You are an evaluator of AI agents. You review task agents' transcripts and decide how to change their prompts.\n",
    "        The task agent's original prompt was: {task_agent_prompt}\n",
    "        The task agent's transcript is: {transcript}\n",
    "        The task agent's evaluation is: {evaluation}\n",
    "\n",
    "        You decide to change the task agent's prompt to:\n",
    "        \"\"\")\n",
    "        chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        chain.run({'task_agent_prompt': task_agent_prompt, 'evaluation': evaluation})\n",
    "\n",
    "    def _arun(self, evaluation: str, transcript: str, prmopt: str):\n",
    "        pass\n",
    "\n",
    "tools = [\n",
    "    EvaluateSingleTranscriptTool(llm),\n",
    "    ModifyPromptTool(llm)\n",
    "]\n",
    "\n",
    "evaluator_agent = OpenAIFunctionsAgent.from_llm_and_tools(\n",
    "    llm=llm, \n",
    "    tools=tools,\n",
    "    prefix=\"You are an evaluator of AI agents. You review task agents' transcripts and decide how to change their prompts.\"\n",
    "    )\n",
    "\n",
    "evaluator_agent_executor = AgentExecutor.from_agent_and_tools(evaluator_agent, tools)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_agent_prefix = open('./task_agent_prefix.txt', 'r').read()\n",
    "task_agent = OpenAIFunctionsAgent.from_llm_and_tools(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prefix=task_agent_prefix\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waller-project-_ugCvovn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
